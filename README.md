# LLM Scraper

A flexible, configuration-driven web scraper designed to extract article content and feed it into a Retrieval-Augmented Generation (RAG) pipeline.

This project uses a professional, scalable architecture with domain-specific parser configurations to accurately extract structured data from web pages.

## ðŸ“š Documentation

**[Complete Documentation â†’](docs/README.md)**

- **[Selector Guide](docs/SELECTOR_GUIDE.md)** - Creating parser configurations
- **[Article API](docs/API_ARTICLE.md)** - Article model reference
- Quick examples and tutorials

## Core Features

- **Config-Driven Parsing**: Define how to scrape any site using simple JSON configuration files
- **Flexible Selectors**: Support for fallback chains, parent scoping, and per-selector attributes
- **Rich Metadata**: Extracts OpenGraph, Schema.org, authors, dates, tags, and topics
- **RAG-Ready**: Automatically chunks content with token estimation for LLM context windows
- **Production-Ready**: Pydantic validation, error handling, deterministic UUIDs

## Project Structure

```
.
â”œâ”€â”€ Procfile              # Defines processes for Honcho (api, worker, beat)
â”œâ”€â”€ api.py                # FastAPI application, the user-facing entrypoint
â”œâ”€â”€ celery_app.py         # Celery application instance configuration
â”œâ”€â”€ pyproject.toml        # Project metadata and dependencies
â”œâ”€â”€ src/
â”‚   â””â”€â”€ llm_scraper/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ articles.py   # Core Article data model and chunking logic
â”‚       â”œâ”€â”€ meta.py       # Metadata extraction logic
â”‚       â”œâ”€â”€ parsers/      # Site-specific parser configurations
â”‚       â”œâ”€â”€ schema.py     # Pydantic models for configuration and data
â”‚       â”œâ”€â”€ settings.py   # Application settings management (from .env)
â”‚       â”œâ”€â”€ utils.py      # Utility functions
â”‚       â””â”€â”€ vector_store.py # Handles interaction with OpenAI and AstraDB
â””â”€â”€ worker.py             # Celery worker and scheduler (Celery Beat) definitions
```

## Setup

1.  **Install Dependencies**: This project uses `uv` for package management.
    ```bash
    uv pip install -r requirements.txt
    ```

2.  **Environment Variables**: Create a `.env` file in the root directory and add your credentials:
    ```env
    # .env
    OPENAI_API_KEY="sk-..."
    ASTRA_DB_APPLICATION_TOKEN="AstraCS:..."
    ASTRA_DB_API_ENDPOINT="https://..."
    ASTRA_DB_COLLECTION_NAME="your_collection_name"
    REDIS_URL="redis://localhost:6379/0"
    ```

3.  **Run Redis**: Ensure you have a Redis server running locally. You can use Docker for this:
    ```bash
    docker run -d -p 6379:6379 redis
    ```

## How to Run

You can run the system in two ways: locally using `honcho` or with Docker.

### 1. Running with Docker (Recommended)

This is the easiest way to run the entire system, including the Redis database.

**Prerequisites**:
- Docker and Docker Compose installed.
- A `.env` file with your credentials (see Setup section).

**To start the entire system, run:**
```bash
docker-compose up --build
```
This command will:
1.  Build the Docker image for the application based on the `Dockerfile`.
2.  Start containers for the `api`, `worker`, `beat`, and `redis` services.
3.  Display all logs in your terminal.

To stop the services, press `Ctrl+C`.

### 2. Running Locally with Honcho

Use this method if you prefer not to use Docker.

**Prerequisites**:
- Python and `uv` installed.
- A running Redis server (e.g., `docker run -d -p 6379:6379 redis`).
- Dependencies installed (`uv pip install -r requirements.txt`).
- A `.env` file with your credentials.

**To start the entire system, run:**
```bash
honcho start
```

## API Usage

-   **`POST /scrape-url`**: Scrape a single URL on-demand.
-   **`POST /scrape-site`**: Trigger a background task to scrape an entire pre-configured site.
-   **`GET /tasks/{task_id}`**: Check the status of a background task.
-   **`POST /query`**: Perform a similarity search on the vectorized data in AstraDB.
